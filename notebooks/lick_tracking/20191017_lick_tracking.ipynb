{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut Toolbox\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Nath\\*, Mathis\\* et al. *Using DeepLabCut for markerless pose estimation during behavior across species*, (under revision).\n",
    "\n",
    "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
    "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
    "\n",
    "This notebook illustrates how to:\n",
    "- create a project\n",
    "- extract training frames\n",
    "- label the frames\n",
    "- plot the labeled images\n",
    "- create a training set\n",
    "- train a network\n",
    "- evaluate a network\n",
    "- analyze a novel video\n",
    "- create an automatically labeled video \n",
    "- plot the trajectories\n",
    "\n",
    "*Note*: Refine a network based after the network was trained on just a few labeled images is illustrated in \"Demo-labeledexample-MouseReaching.ipynb\". This demo also contains an already labeled data set and is perhaps the best starting point for brand new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Create a new project\n",
    "\n",
    "It is always good idea to keep the projects seperate. This function creates a new project with subdirectories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory.\n",
    "\n",
    "You can always add new videos to the project at any stage of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLZhp7EoEI0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jojo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jojo/DeepLabCut-master/deeplabcut/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videofile_path = ['/home/jojo/DeepLabCut-master/videos/lick_tracking'] \n",
    "path_config_file =['/home/jojo/DeepLabCut-master/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/videos\"\n",
      "Created \"/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/labeled-data\"\n",
      "Created \"/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/training-datasets\"\n",
      "Created \"/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/dlc-models\"\n",
      "Creating the symbolic link of the video\n",
      "Created the symlink of /home/jojo/DeepLabCut-master/videos/lick_tracking/20191017_Lotus_01_lineartrack.mp4 to /home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/videos/20191017_Lotus_01_lineartrack.mp4\n",
      "/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/videos/20191017_Lotus_01_lineartrack.mp4\n",
      "Generated \"/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/config.yaml\"\n",
      "\n",
      "A new project with name lineartrack-DR-2019-10-17 is created at /home/jojo/DeepLabCut-master/working_directory and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jojo/DeepLabCut-master/working_directory/lineartrack-DR-2019-10-17/config.yaml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task='lineartrack' # Enter the name of your experiment Task\n",
    "experimenter='DR' # Enter the name of the experimenter\n",
    "video=['/home/jojo/DeepLabCut-master/videos/lick_tracking/20191017_Lotus_01_lineartrack.mp4']\n",
    "deeplabcut.create_new_project(task,experimenter,video, working_directory='working_directory',copy_videos=False) #change the working directory to where you want the folders created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yXW0bx1oEJA"
   },
   "source": [
    "## Extract frames from videos \n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) (algo=='uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ulumCuoEJC",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MainFrame' object has no attribute 'new_x1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/generate_training_dataset/frame_extraction_toolbox.py\u001b[0m in \u001b[0;36mis_crop_ok\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumberFrames\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumberFrames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_x1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_x2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_y1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MainFrame' object has no attribute 'new_x1'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting for now!\n",
      "\n",
      "Frames were selected.\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "path_config_file = '/home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/config.yaml'  # Enter the path of the config file that was just created from the above step (check the folder)\n",
    "deeplabcut.extract_frames(path_config_file,'manual',crop=True) #there are other ways to grab frames, such as by clustering 'kmeans'; please see the paper. \n",
    "#You can change the cropping to false, then delete the checkcropping part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gjn6ZDonoEJH"
   },
   "source": [
    "## Label the extracted frames\n",
    "Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory **'labeled-data'**. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyROSOiEoEJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "path_config_file = '/home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/config.yaml'  # Enter the path of the config file that was just created from the above step (check the folder)\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vim95ZvkPSeN"
   },
   "source": [
    "**Check the labels**\n",
    "\n",
    "Checking if the labels were created and stored correctly is beneficial for training, since labeling is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function `check\\_labels'  to do so. It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwvgPJouPP2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by YM.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190826_Jaq_02_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190826_Jaq_04_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190826_Jaq_06_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Jaq_02_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Jaq_04_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Jaq_06_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Jaq_08_lineartrack_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Lotus_02_linear1_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Lotus_04_linear2_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Lotus_06_linear3_labeled.\n",
      "They are stored in the following folder: /home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/labeled-data/20190827_Lotus_08_linear4_labeled.\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    }
   ],
   "source": [
    "path_config_file = '/home/jojo/DeepLabCut-master/working_directory/lineartrack-YM-2019-08-28/config.yaml'   # Enter the path of the config file that was just created from the above step (check the folder)\n",
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "of87fOjgPqzH"
   },
   "source": [
    "If the labels need adjusted, you can use the refinement GUI to move them around! Check that out below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "This function generates the training data information for DeepCut (which requires a mat file) based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles. \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
    "\n",
    "Now it is the time to start training the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training - If you want to use a CPU, continue. \n",
    "### If yu want to use your GPU, you need to exit here and either work from the Docker container, your own TensorFlow installation in an Anaconda env\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Start evaluating\n",
    "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Start Analyzing videos\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-150000 for model /home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/dlc-models/iteration-0/lineartrackApr4-trainset95shuffle1\n",
      "INFO:tensorflow:Restoring parameters from /home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/dlc-models/iteration-0/lineartrackApr4-trainset95shuffle1/train/snapshot-150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/dlc-models/iteration-0/lineartrackApr4-trainset95shuffle1/train/snapshot-150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4\n",
      "Loading  /home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/115174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of video [s]:  921.39 , recorded with  125.0 fps!\n",
      "Overall # of frames:  115174  found with (before cropping) frame dimensions:  1300 200\n",
      "Starting to extract posture\n",
      "Cropping based on the x1 = 0 x2 = 1300 y1 = 0 y2 = 120. You can adjust the cropping coordinates in the config.yaml file.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/pose_estimation_tensorflow/predict_videos.py\u001b[0m in \u001b[0;36mAnalyzeVideo\u001b[0;34m(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, destfolder)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Attempt to load data...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video already analyzed!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             raise compat.FileNotFoundError(\n\u001b[0;32m--> 347\u001b[0;31m                 'File %s does not exist' % path_or_buf)\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrackDeepCut_resnet50_lineartrackApr4shuffle1_150000.h5 does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-88a5384ca0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_config_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/config.yaml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideofile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_04_lineartrack.mp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_06_lineartrack.mp4'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#Enter the list of videos to analyze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideofile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_as_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideotype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/pose_estimation_tensorflow/predict_videos.py\u001b[0m in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, destfolder)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m#looping over videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mVideos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mAnalyzeVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDLCscorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainFraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_as_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/pose_estimation_tensorflow/predict_videos.py\u001b[0m in \u001b[0;36mAnalyzeVideo\u001b[0;34m(video, DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, destfolder)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting to extract posture\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mPredicteData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGetPoseF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mPredicteData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGetPoseS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/pose_estimation_tensorflow/predict_videos.py\u001b[0m in \u001b[0;36mGetPoseF\u001b[0;34m(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_ind\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                     \u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetposeNP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                     \u001b[0mPredicteData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mbatch_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/DeepLabCut-master/deeplabcut/pose_estimation_tensorflow/nnet/predict.py\u001b[0m in \u001b[0;36mgetposeNP\u001b[0;34m(image, cfg, sess, inputs, outputs, outall)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetposeNP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m''' Adapted from DeeperCut, performs numpy-based faster inference on batches'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0moutputs_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mscmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_cnn_outputmulti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#processes image batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jojo/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_config_file='/home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/config.yaml'\n",
    "videofile_path = ['/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_04_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_06_lineartrack.mp4']  #Enter the list of videos to analyze\n",
    "deeplabcut.analyze_videos(path_config_file, videofile_path, save_as_csv=True, videotype='.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGu_PdTWoEJr"
   },
   "source": [
    "## Extract outlier frames [optional step]\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. Make sure to provide the correct value of the \"iterations\" as it will be used to create the unique directory where the extracted frames will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkbaBOJVoEJs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network parameters: DeepCut_resnet50_lineartrackApr4shuffle1_150000\n",
      "Method  jump  found  10128  putative outlier frames.\n",
      "Do you want to proceed with extracting  100  of those?\n",
      "If this list is very large, perhaps consider changing the paramters (start, stop, epsilon, comparisonbodyparts) or use a different method.\n",
      "yes/noyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames from video 20190320_aj69_02_lineartrack  already extracted (more will be added)!\n",
      "Loading video...\n",
      "Duration of video [s]:  921.3874999999999 , recorded @  125.00061049232815 fps!\n",
      "Overall # of frames:  115174 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 921.39  seconds.\n",
      "Extracting and downsampling... 10128  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10128it [30:42,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [51565, 56099, 63221, 9240, 96730, 93155, 98837, 86661, 105459, 3829, 73194, 54014, 79992, 50700, 83335, 96856, 49098, 93140, 64997, 37433, 106633, 58935, 76875, 58984, 95505, 54120, 49003, 64445, 63222, 58250, 64639, 83211, 46374, 110762, 73042, 86724, 23685, 80010, 68295, 23744, 39916, 75799, 17587, 90712, 30830, 105149, 92273, 49026, 77064, 37013, 36465, 90834, 110963, 56344, 64467, 46484, 56232, 83221, 51517, 96854, 49046, 58719, 40627, 26903, 105040, 113486, 81271, 85267, 47944, 42961, 54048, 51487, 113306, 73235, 15991, 93192, 88237, 71495, 69647, 78276, 69592, 83426, 67515, 83444, 16303, 43349, 86742, 50503, 100413, 83297, 8890, 30704, 44293, 43462, 108871, 39896, 111160, 51411, 105002, 69671]\n",
      "New video was added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\20190320_aj69_02_lineartrack.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n",
      "Method  jump  found  12326  putative outlier frames.\n",
      "Do you want to proceed with extracting  100  of those?\n",
      "If this list is very large, perhaps consider changing the paramters (start, stop, epsilon, comparisonbodyparts) or use a different method.\n",
      "yes/noyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames from video 20190320_aj69_04_lineartrack  already extracted (more will be added)!\n",
      "Loading video...\n",
      "Duration of video [s]:  1004.93575 , recorded @  125.00003109651537 fps!\n",
      "Overall # of frames:  125617 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 1004.94  seconds.\n",
      "Extracting and downsampling... 12326  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12326it [38:35,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [69928, 68870, 115387, 54943, 103603, 70586, 58974, 73930, 57347, 79347, 54735, 104522, 42661, 110168, 83103, 40773, 120493, 99237, 109102, 49910, 112139, 70089, 107383, 109192, 83049, 74486, 124126, 106089, 67856, 44678, 68875, 42631, 41302, 53592, 48737, 32307, 97844, 47551, 103571, 89845, 65505, 87413, 30725, 71430, 95351, 84076, 106161, 118136, 75161, 104526, 2970, 68934, 106257, 89912, 59027, 57884, 79790, 32805, 24921, 35214, 73477, 124678, 111945, 24701, 27332, 75124, 57927, 106303, 93467, 113102, 112292, 89221, 113065, 46243, 74829, 50697, 53535, 53603, 80845, 118285, 24531, 101468, 53659, 104583, 53276, 44933, 118318, 54812, 68909, 99168, 10746, 91611, 116288, 39605, 94418, 87553, 78830, 67660, 72041, 102502]\n",
      "New video was added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\20190320_aj69_04_lineartrack.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n",
      "Method  jump  found  8230  putative outlier frames.\n",
      "Do you want to proceed with extracting  100  of those?\n",
      "If this list is very large, perhaps consider changing the paramters (start, stop, epsilon, comparisonbodyparts) or use a different method.\n",
      "yes/nono\n",
      "Nothing extracted, change parameters and start again...\n"
     ]
    }
   ],
   "source": [
    "path_config_file='/home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/config.yaml'\n",
    "videofile_path = ['/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_04_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_06_lineartrack.mp4']  #Enter the list of videos to analyze\n",
    "deeplabcut.extract_outlier_frames(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ib0uvhaoEJx"
   },
   "source": [
    "## Refine Labels [optional step]\n",
    "Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_FpEXtyoEJy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cannot activate multiple GUI eventloops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n",
      "Checking labels if they are outside the image\n",
      "A training dataset file is already found for this video. The refined machine labels are merged to this data!\n",
      "Linux\n",
      "Checking labels if they are outside the image\n",
      "Found nose outside the image labeled-data/20190320_aj69_04_lineartrack/img072041.png.Setting it to NaN\n",
      "A training dataset file is already found for this video. The refined machine labels are merged to this data!\n",
      "Closing... The refined labels are stored in a subdirectory under labeled-data. Use the function 'merge_datasets' to augment the training dataset, and then re-train a network using create_training_dataset followed by train_network!\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "path_config_file='/home/jojo/DeepLabCut-master/working_directory/lineartrack-AJ-2019-04-04/config.yaml'\n",
    "videofile_path = ['/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_02_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_04_lineartrack.mp4','/home/jojo/DeepLabCut-master/videos/20190320_aj69/20190320_aj69_06_lineartrack.mp4']  #Enter the list of videos to analyze\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHzstWr8oEJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 1.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n"
     ]
    }
   ],
   "source": [
    "#Once all folders are relabeled, check them and advance. See how to check labels, above!\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "Following the refine labels, append these frames to the original dataset to create a new iteration of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path,save_frames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #for making interactive plots.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "#import sys\n",
    "import argparse, glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "if os.environ.get('DLClight', default=False) == 'True':\n",
    "    mpl.use('AGG') #anti-grain geometry engine #https://matplotlib.org/faq/usage_faq.html\n",
    "else:\n",
    "    mpl.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deeplabcut.utils import auxiliaryfunctions\n",
    "from deeplabcut.pose_estimation_tensorflow.config import load_config\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.draw import circle_perimeter, circle\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV as vp # used to CreateVideo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io; io.use_plugin('matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=480\n",
    "y1=100\n",
    "x2=200\n",
    "x1=0\n",
    "nframes= 100\n",
    "start=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords =[x1,x2,y1,y2]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d308d43cef37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mymlfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "config_file = Path(config).resolve()\n",
    "\n",
    "\n",
    "\t\n",
    "with open(str(config_file), 'r') as ymlfile:\n",
    "\t  \n",
    "      cfg = yaml.load(ymlfile)  \n",
    "      videos = cfg['video_sets'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crop(video,start,coords):\n",
    "\n",
    "    \"\"\"\n",
    "    --------\n",
    "    coord is a list\n",
    "    start in second\n",
    "    video is the path of the video?? in \" .mp4\"\n",
    "\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import yaml\n",
    "    import numpy as np\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    from pathlib import Path\n",
    "    from skimage import io\n",
    "    from skimage.util import img_as_ubyte\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    from deeplabcut.generate_training_dataset import frameselectiontools\n",
    "\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    # update to openCV\n",
    "    clip = VideoFileClip(video)\n",
    "    image = clip.get_frame(start)  # frame is accessed by index *1./clip.fps (fps cancels)\n",
    "    fname = Path(video)\n",
    "   # output_path = Path(config).parents[0] / 'labeled-data' / fname.stem\n",
    "\n",
    "    fig ,ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((int(coords[0]),int(coords[2])),int(coords[1]),int(coords[3]),linewidth=3,edgecolor='r',facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"The red boundary indicates how the cropped image will look.\")\n",
    "\n",
    "\n",
    "    # crop and move on with extraction of frames:\n",
    "    # clip = clip.crop(y1=int(coords[2]), y2=int(coords[3]), x1=int(coords[0]), x2=int(coords[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_crop(video,start,coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_crop('/home/jojo/DeepLabCut-master/video_crop_cut/121318_65_0_CC.mp4',10,coords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVideo_CutCrop(videos,x1,x2,y1,y2,nframes,videotype='avi',save_frames=True,delete=True):\n",
    "\n",
    "    \"\"\"\n",
    "    videos : list\n",
    "        A list of string containing the full paths of the videos to analyze.    \n",
    "    videotype: string, optional\n",
    "        Checks for the extension of the video in case the input is a directory.\\nOnly videos with this extension are analyzed. The default is ``.avi``\n",
    "    save_frames: bool\n",
    "        If true creates each frame individual and then combines into a video. This variant is relatively slow as\n",
    "        it stores all individual frames. However, it uses matplotlib to create the frames and is therefore much more flexible (one can set transparency of markers, crop, and easily customize).\n",
    "    delete: bool\n",
    "        If true then the individual frames created during the video generation will be deleted.\n",
    "   \n",
    "    Examples\n",
    "    --------\n",
    "    If you want to create the labeled video for only 1 video\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi'])\n",
    "    --------\n",
    "    If you want to create the labeled video for only 1 video and store the individual frames\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi'],save_frames=True)\n",
    "    --------\n",
    "    If you want to create the labeled video for multiple videos\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'])\n",
    "    --------\n",
    "    If you want to create the labeled video for all the videos (as .avi extension) in a directory.\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/'])\n",
    "    --------\n",
    "    If you want to create the labeled video for all the videos (as .mp4 extension) in a directory.\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/'],videotype='mp4')\n",
    "    --------\n",
    "    \"\"\"\n",
    "   \n",
    "    ''' Creating individual frames and making a video'''\n",
    "    if [os.path.isdir(i) for i in videos] == [True]:\n",
    "      print(\"Analyzing all the videos in the directory\")\n",
    "      videofolder= videos[0]\n",
    "      os.chdir(videofolder)\n",
    "      Videos = np.sort([fn for fn in os.listdir(os.curdir) if (videotype in fn)])\n",
    "      print(\"Starting \", videofolder, Videos)\n",
    "    else:\n",
    "      Videos = videos\n",
    "\n",
    "    for video in Videos:\n",
    "        videofolder= Path(video).parents[0] #where your folder with videos is.\n",
    "        os.chdir(str(videofolder))\n",
    "        videotype = Path(video).suffix\n",
    "        print(\"Starting % \", videofolder, videos)\n",
    "        vname = str(Path(video).stem)          \n",
    "        print(\"Loading \", video, \"and data.\")\n",
    "\n",
    "        if save_frames==True:\n",
    "            tmpfolder = os.path.join(str(videofolder),'temp-' + vname)\n",
    "            auxiliaryfunctions.attempttomakefolder(tmpfolder)\n",
    "            clip = vp(video)\n",
    "            ny, nx= y2-y1,x2-x1 \n",
    "            fps=clip.fps()\n",
    "            \n",
    "            duration = nframes/fps\n",
    "\n",
    "            print(\"Duration of video [s]: \", round(duration,2), \", recorded with \", round(fps,2),\"fps!\")\n",
    "            print(\"Overall # of frames: \", int(nframes), \"with cropped frame dimensions: \",nx,ny)\n",
    "            print(\"Generating frames\")\n",
    "            for index in tqdm(range(nframes)):\n",
    "                imagename = tmpfolder + \"/file%04d.png\" % index\n",
    "                if os.path.isfile(tmpfolder + \"/file%04d.png\" % index):\n",
    "                    image = img_as_ubyte(clip.load_frame()) #still need to read (so counter advances!)\n",
    "                else:\n",
    "                    plt.axis('off')\n",
    "                    image = img_as_ubyte(clip.load_frame())                   \n",
    "                    image=image[y1:y2,x1:x2]\n",
    "                    \n",
    "                    plt.figure(frameon=False, figsize=(nx * 1. / 100, ny * 1. / 100))\n",
    "                    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "                    plt.imshow(image)\n",
    "\n",
    "\n",
    "                    plt.xlim(0, nx)\n",
    "                    plt.ylim(0, ny)\n",
    "                    plt.axis('off')\n",
    "                    plt.subplots_adjust(\n",
    "                        left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    plt.savefig(imagename)\n",
    "\n",
    "                    plt.close(\"all\")\n",
    "\n",
    "            start= os.getcwd()\n",
    "            os.chdir(tmpfolder)\n",
    "\n",
    "            print(\"All frames were created, now generating video...\")\n",
    "            vname=str(Path(tmpfolder).stem).split('-')[1]\n",
    "            try: ## One can change the parameters of the video creation script below:\n",
    "                subprocess.call([\n",
    "                    'ffmpeg', '-framerate',\n",
    "                    str(clip.fps()), '-i', 'file%04d.png', '-r', '30','../'+vname +'_CC.mp4'])\n",
    "            except FileNotFoundError:\n",
    "                print(\"Ffmpeg not correctly installed, see https://github.com/AlexEMG/DeepLabCut/issues/45\")\n",
    "\n",
    "            if delete:\n",
    "                for file_name in glob.glob(\"*.png\"):\n",
    "                    os.remove(file_name)\n",
    "            os.chdir(start)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateVideo_CutCrop(['/home/jojo/DeepLabCut-master/video_crop_cut/121318_65_0.1.h264'],x1,x2,y1,y2,nframes,videotype='h264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVideo_CutCrop(videos,x1,x2,y1,y2,coords,nframes,videotype='avi',save_frames=True,delete=True):\n",
    "\n",
    "    \"\"\"\n",
    "    videos : list\n",
    "        A list of string containing the full paths of the videos to analyze.    \n",
    "    videotype: string, optional\n",
    "        Checks for the extension of the video in case the input is a directory.\\nOnly videos with this extension are analyzed. The default is ``.avi``\n",
    "    save_frames: bool\n",
    "        If true creates each frame individual and then combines into a video. This variant is relatively slow as\n",
    "        it stores all individual frames. However, it uses matplotlib to create the frames and is therefore much more flexible (one can set transparency of markers, crop, and easily customize).\n",
    "    delete: bool\n",
    "        If true then the individual frames created during the video generation will be deleted.\n",
    "   \n",
    "    Examples\n",
    "    --------\n",
    "    If you want to create the labeled video for only 1 video\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi'])\n",
    "    --------\n",
    "    If you want to create the labeled video for only 1 video and store the individual frames\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi'],save_frames=True)\n",
    "    --------\n",
    "    If you want to create the labeled video for multiple videos\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'])\n",
    "    --------\n",
    "    If you want to create the labeled video for all the videos (as .avi extension) in a directory.\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/'])\n",
    "    --------\n",
    "    If you want to create the labeled video for all the videos (as .mp4 extension) in a directory.\n",
    "    >>> deeplabcut.create_labeled_video(['/analysis/project/videos/'],videotype='mp4')\n",
    "    --------\n",
    "    \"\"\"\n",
    "   \n",
    "    ''' Creating individual frames and making a video'''\n",
    "    if [os.path.isdir(i) for i in videos] == [True]:\n",
    "      print(\"Analyzing all the videos in the directory\")\n",
    "      videofolder= videos[0]\n",
    "      os.chdir(videofolder)\n",
    "      Videos = np.sort([fn for fn in os.listdir(os.curdir) if (videotype in fn)])\n",
    "      print(\"Starting \", videofolder, Videos)\n",
    "    else:\n",
    "      Videos = videos\n",
    "\n",
    "    for video in Videos:\n",
    "        videofolder= Path(video).parents[0] #where your folder with videos is.\n",
    "        os.chdir(str(videofolder))\n",
    "        videotype = Path(video).suffix\n",
    "        print(\"Starting % \", videofolder, videos)\n",
    "        vname = str(Path(video).stem)          \n",
    "        print(\"Loading \", video, \"and data.\")\n",
    "\n",
    "        if save_frames==True:\n",
    "            tmpfolder = os.path.join(str(videofolder),'temp-' + vname)\n",
    "            auxiliaryfunctions.attempttomakefolder(tmpfolder)\n",
    "            clip = vp(video)\n",
    "            ny, nx= y2-y1,x2-x1 \n",
    "            fps=clip.fps()\n",
    "            \n",
    "            duration = nframes/fps\n",
    "\n",
    "            print(\"Duration of video [s]: \", round(duration,2), \", recorded with \", round(fps,2),\"fps!\")\n",
    "            print(\"Overall # of frames: \", int(nframes), \"with cropped frame dimensions: \",nx,ny)\n",
    "            print(\"Generating frames\")\n",
    "            for index in tqdm(range(nframes)):\n",
    "                image = clip.get_frame(start)  # frame is accessed by index *1./clip.fps (fps cancels)\n",
    "                \n",
    "                fig ,ax = plt.subplots(1)\n",
    "                # Display the image\n",
    "                ax.imshow(image)\n",
    "                # Create a Rectangle patch\n",
    "                rect = patches.Rectangle((int(coords[0]),int(coords[2])),int(coords[1]),int(coords[3]),linewidth=3,edgecolor='r',facecolor='none')\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "                plt.show()\n",
    "\n",
    "                print(\"The red boundary indicates how the cropped image will look.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateVideo_CutCrop(['/home/jojo/DeepLabCut-master/video_crop_cut/121318_65_0.1.h264'],x1,x2,y1,y2,coords,nframes,videotype='h264',save_frames=True,delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
